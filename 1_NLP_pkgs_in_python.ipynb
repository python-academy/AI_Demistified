{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_NLP_pkgs_in_python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/python-academy/AI_Demistified/blob/master/1_NLP_pkgs_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PWzkYMZShDd"
      },
      "source": [
        "# GitHub https://github.com/python-academy/NLP-Packages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ilbN75_h2t"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcjtw4jNGnvi"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE026ZBkGzdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c81a80da-da83-4b84-f9c8-4246a84f1147"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AELDg7kmHKoE"
      },
      "source": [
        "from nltk.corpus import twitter_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIFWZ6R-HT8E"
      },
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tfGL0YcIIVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bd13a0e-f96b-40bb-9786-3e07bb19ecde"
      },
      "source": [
        "len(positive_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TahgCVB2IW03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5907c5f6-7741-4d63-eb32-9a0a8a7ceab6"
      },
      "source": [
        "len(negative_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIcwgO_MKcCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5ec1a4b2-f221-4de9-edaf-fd3a872e758f"
      },
      "source": [
        "positive_tweets[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAgaLAwlKuz8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e0853c00-9439-421b-b210-8299831a7d14"
      },
      "source": [
        "negative_tweets[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless for tmr :(',\n",
              " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
              " '@Hegelbon That heart sliding into the waste basket. :(',\n",
              " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
              " 'Dang starting next week I have \"work\" :(']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc33iorHIr7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "66610b21-e512-452b-b676-368d5e9370f0"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CakrjBfEIu0s"
      },
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0p3hIYYJxBC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8a7b7e0-a2c6-4937-cec2-350eefa0cd94"
      },
      "source": [
        "tweet_tokens[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#FollowFriday',\n",
              "  '@France_Inte',\n",
              "  '@PKuchly57',\n",
              "  '@Milipol_Paris',\n",
              "  'for',\n",
              "  'being',\n",
              "  'top',\n",
              "  'engaged',\n",
              "  'members',\n",
              "  'in',\n",
              "  'my',\n",
              "  'community',\n",
              "  'this',\n",
              "  'week',\n",
              "  ':)'],\n",
              " ['@Lamb2ja',\n",
              "  'Hey',\n",
              "  'James',\n",
              "  '!',\n",
              "  'How',\n",
              "  'odd',\n",
              "  ':/',\n",
              "  'Please',\n",
              "  'call',\n",
              "  'our',\n",
              "  'Contact',\n",
              "  'Centre',\n",
              "  'on',\n",
              "  '02392441234',\n",
              "  'and',\n",
              "  'we',\n",
              "  'will',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'assist',\n",
              "  'you',\n",
              "  ':)',\n",
              "  'Many',\n",
              "  'thanks',\n",
              "  '!'],\n",
              " ['@DespiteOfficial',\n",
              "  'we',\n",
              "  'had',\n",
              "  'a',\n",
              "  'listen',\n",
              "  'last',\n",
              "  'night',\n",
              "  ':)',\n",
              "  'As',\n",
              "  'You',\n",
              "  'Bleed',\n",
              "  'is',\n",
              "  'an',\n",
              "  'amazing',\n",
              "  'track',\n",
              "  '.',\n",
              "  'When',\n",
              "  'are',\n",
              "  'you',\n",
              "  'in',\n",
              "  'Scotland',\n",
              "  '?',\n",
              "  '!'],\n",
              " ['@97sides', 'CONGRATS', ':)'],\n",
              " ['yeaaaah',\n",
              "  'yippppy',\n",
              "  '!',\n",
              "  '!',\n",
              "  '!',\n",
              "  'my',\n",
              "  'accnt',\n",
              "  'verified',\n",
              "  'rqst',\n",
              "  'has',\n",
              "  'succeed',\n",
              "  'got',\n",
              "  'a',\n",
              "  'blue',\n",
              "  'tick',\n",
              "  'mark',\n",
              "  'on',\n",
              "  'my',\n",
              "  'fb',\n",
              "  'profile',\n",
              "  ':)',\n",
              "  'in',\n",
              "  '15',\n",
              "  'days']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpfJ1RlcLtsq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d630ee14-bb1e-4d61-e925-ee5ef83334d5"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTPQ22D1MC6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "87767736-003f-4a3c-cb84-510325cebc72"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XJmlHAuMGQO"
      },
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiasd7HMLol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "44029d67-b40b-48d5-b4ec-69060da8cf9c"
      },
      "source": [
        "pos_tag(tweet_tokens[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#FollowFriday', 'JJ'),\n",
              " ('@France_Inte', 'NNP'),\n",
              " ('@PKuchly57', 'NNP'),\n",
              " ('@Milipol_Paris', 'NNP'),\n",
              " ('for', 'IN'),\n",
              " ('being', 'VBG'),\n",
              " ('top', 'JJ'),\n",
              " ('engaged', 'VBN'),\n",
              " ('members', 'NNS'),\n",
              " ('in', 'IN'),\n",
              " ('my', 'PRP$'),\n",
              " ('community', 'NN'),\n",
              " ('this', 'DT'),\n",
              " ('week', 'NN'),\n",
              " (':)', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsRDYx9MSJ9"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226NGcp0MiAV"
      },
      "source": [
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8AZ29EKMops",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "29f12ad0-a206-4d37-931b-ab13a9997e47"
      },
      "source": [
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHJBj6iuNCNe"
      },
      "source": [
        "import re, string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czZDIAP_NDve"
      },
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KSc3_meNeOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c33242f7-77c6-4ec3-b9f7-672c4bd8f1bc"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ozfx80zNmnH"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoLIh7fPNsya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fe8ce97-9c5a-421c-c2bc-f91f258de981"
      },
      "source": [
        "remove_noise(tweet_tokens[0], stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9JNbkW6ODfI"
      },
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sbdz3X-OFcO"
      },
      "source": [
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cV5uPwKOJU7"
      },
      "source": [
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6alXqtuOPFb"
      },
      "source": [
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr9nPRh3OUQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6439af11-8741-45c0-f0dc-fdc42f374994"
      },
      "source": [
        "positive_cleaned_tokens_list[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)'],\n",
              " ['hey',\n",
              "  'james',\n",
              "  'odd',\n",
              "  ':/',\n",
              "  'please',\n",
              "  'call',\n",
              "  'contact',\n",
              "  'centre',\n",
              "  '02392441234',\n",
              "  'able',\n",
              "  'assist',\n",
              "  ':)',\n",
              "  'many',\n",
              "  'thanks'],\n",
              " ['listen', 'last', 'night', ':)', 'bleed', 'amazing', 'track', 'scotland']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-oFtygOeHT"
      },
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5p8HjvJO9E8"
      },
      "source": [
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMaGJOtTPB3y"
      },
      "source": [
        "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik9iUmXsPIK2"
      },
      "source": [
        "from nltk import FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDEtQRnNPPTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "095a8689-092b-41c1-e720-40c4e188fdc9"
      },
      "source": [
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "freq_dist_pos.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(':)', 3691),\n",
              " (':-)', 701),\n",
              " (':d', 658),\n",
              " ('thanks', 388),\n",
              " ('follow', 357),\n",
              " ('love', 333),\n",
              " ('...', 290),\n",
              " ('good', 283),\n",
              " ('get', 263),\n",
              " ('thank', 253)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccz9_-GmPVrk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "655799ae-33bb-4d90-d45d-5585bc76143d"
      },
      "source": [
        "freq_dist_neg = FreqDist(all_neg_words)\n",
        "freq_dist_neg.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(':(', 4585),\n",
              " (':-(', 501),\n",
              " (\"i'm\", 343),\n",
              " ('...', 332),\n",
              " ('get', 325),\n",
              " ('miss', 291),\n",
              " ('go', 275),\n",
              " ('please', 275),\n",
              " ('want', 246),\n",
              " ('like', 218)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFyrmCbtQJl-"
      },
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnVwBGaJknDR"
      },
      "source": [
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_aGuzPwQIN6"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Nj1yX6QTak"
      },
      "source": [
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdT0jRxxQcPr"
      },
      "source": [
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0aS4tr0SOPM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "746f2bd5-e553-48b9-ca12-eaf4146096d9"
      },
      "source": [
        "positive_dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'#followfriday': True,\n",
              "  ':)': True,\n",
              "  'community': True,\n",
              "  'engage': True,\n",
              "  'member': True,\n",
              "  'top': True,\n",
              "  'week': True},\n",
              " 'Positive')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTsgflsQfzr"
      },
      "source": [
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxfiGW5lQlAb"
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIvbX-dwQp4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3208a5cd-647e-4751-949d-4db30cb7a3ea"
      },
      "source": [
        "classify.accuracy(classifier, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9956666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYTxUgVWQwZq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "99e1c382-6e8a-459e-9fb0-feb8160c7ddd"
      },
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2081.2 : 1.0\n",
            "                      :) = True           Positi : Negati =    982.9 : 1.0\n",
            "                     sad = True           Negati : Positi =     24.2 : 1.0\n",
            "                follower = True           Positi : Negati =     23.9 : 1.0\n",
            "                     x15 = True           Negati : Positi =     17.2 : 1.0\n",
            "               community = True           Positi : Negati =     16.8 : 1.0\n",
            "                 welcome = True           Positi : Negati =     15.1 : 1.0\n",
            "                      aw = True           Negati : Positi =     14.5 : 1.0\n",
            "                  arrive = True           Positi : Negati =     14.5 : 1.0\n",
            "              appreciate = True           Positi : Negati =     14.1 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPCpr_WFQ5Cs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70ed08e6-9e0d-4f64-c396-5cba1dee4465"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, \\       \n",
        "                    never used the app again.\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "classifier.classify(dict([token, True] for token in custom_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVscz95yF10y"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13VN5EPIyj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "4d62dfbb-cc1e-46dd-bc97-55b6e094ac93"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-esXoDkmfTY1"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1UszuCqfWR_"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaGO3XB0F1Xn"
      },
      "source": [
        "f= open('/content/drive/My Drive/model.pkl','wb')\n",
        "pickle.dump(classifier,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_18FwSIKF04l"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ov-HWawF0TL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-81pO1sFzeM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4jHwRr4Aa1n"
      },
      "source": [
        "# **TextBlob**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMZCDkNDPdY"
      },
      "source": [
        "*1.Tokenization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V57Aa3lOD93T"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ngN84fAg2e"
      },
      "source": [
        "positive_tweets[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pBnRv3_Ddrx"
      },
      "source": [
        "blob = TextBlob(positive_tweets[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN3tPyZzES09"
      },
      "source": [
        "blob.sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC45D-NbEh5v"
      },
      "source": [
        "for words in blob.sentences[0].words:  \n",
        "  print (words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXKkmqgEzgL"
      },
      "source": [
        "2. Part-of-speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si--X1t6E5Pp"
      },
      "source": [
        "for words, tag in blob.tags:\n",
        " print (words, tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSsr7YB2Lkfn"
      },
      "source": [
        "3. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VzCwx0RJwoE"
      },
      "source": [
        "for words in blob.sentences[0].words:  \n",
        "  print (words , words.lemmatize(\"v\") )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGbLVDUiLupC"
      },
      "source": [
        "4.Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP5ygxZ2Lx0o"
      },
      "source": [
        "def remove_noise(tweets_list, stop_words = ()):\n",
        "    cleaned_tokens = []\n",
        "    for tweet in tweets_list:\n",
        "      blob = TextBlob(tweet)\n",
        "      for words, tag in blob.tags:\n",
        "        if tag.startswith(\"NN\"):pos = 'n'\n",
        "        elif tag.startswith('VB'):pos = 'v'\n",
        "        else:pos = 'a'\n",
        "\n",
        "        words = words.lemmatize(pos)\n",
        "        words = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                          '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', words)\n",
        "        words = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", words)\n",
        "\n",
        "        if len(words) > 0 and words not in string.punctuation and words.lower() not in stop_words:\n",
        "                cleaned_tokens.append(words.lower())\n",
        "    return cleaned_tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4hnbfLbTxLc"
      },
      "source": [
        "positive_cleaned_tokens_list = remove_noise(positive_tweets,stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P9ObTZBVZb8"
      },
      "source": [
        "negative_cleaned_tokens_list = remove_noise(negative_tweets,stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o4CuU8oUzZu"
      },
      "source": [
        "positive_cleaned_tokens_list[:5]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWCrKyNfVhGn"
      },
      "source": [
        "negative_cleaned_tokens_list[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAmvpjgCWCLH"
      },
      "source": [
        "positive_dataset = [(tweet_token, \"Positive\")\n",
        "                     for tweet_token in positive_cleaned_tokens_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNLvHGNPWPY-"
      },
      "source": [
        "negative_dataset = [(tweet_token, \"Negative\")\n",
        "                     for tweet_token in negative_cleaned_tokens_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZl0gT79VQmt"
      },
      "source": [
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ol9TemoWeqQ"
      },
      "source": [
        "from textblob import classifiers\n",
        "classifier = classifiers.NaiveBayesClassifier(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDI2irczyvkS"
      },
      "source": [
        "custom_tweet = [\"I ordered just once from TerribleCo, they screwed up, never used the app again.\"]\n",
        "\n",
        "custom_tokens = remove_noise(custom_tweet,stop_words)\n",
        "\n",
        "print(custom_tokens)\n",
        "\n",
        "classifier.classify(custom_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHKk0j62Pjqz"
      },
      "source": [
        "# 3. spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgaBn-TT9ea0"
      },
      "source": [
        "new_positive_list = [[tweet , 1 ] for tweet in positive_tweets]\n",
        "new_negative_list = [[tweet , 0] for tweet in negative_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7-4mBth_MKI"
      },
      "source": [
        "list_to_df = new_positive_list + new_negative_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeHFTzos-DWJ"
      },
      "source": [
        "list_to_df[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reicvMuu_eHU"
      },
      "source": [
        "list_to_df[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOFmxKzl-Ld6"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHZqCfNp-SRK"
      },
      "source": [
        "df_text_sentiment = pd.DataFrame(list_to_df , columns =['Text', 'Sentiment']  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfzoEjdj-wPF"
      },
      "source": [
        "df_text_sentiment.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yG30NlxAFkR"
      },
      "source": [
        "df_text_sentiment.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1A5yR1RAr0a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeQsE7F-Aml2"
      },
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b8_qEakquNb"
      },
      "source": [
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnSGpjV-qle_"
      },
      "source": [
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create \n",
        "    # documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() \n",
        "                if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens \n",
        "                if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0zYLqUUA80Z"
      },
      "source": [
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2kVHWpIqdrU"
      },
      "source": [
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl7QAsJ6BD6W"
      },
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCDx2xbOBKZm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_text_sentiment['Text'] # the features we want to analyze\n",
        "ylabels = df_text_sentiment['Sentiment'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T6lzOGsCJkV"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(solver = 'lbfgs')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M5uVngrCV7_"
      },
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI5uCrBmHBwM"
      },
      "source": [
        "predicted[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnUvWeHjHfRK"
      },
      "source": [
        "y_test[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF9FF4I1Hp8I"
      },
      "source": [
        "X_test[:5]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}